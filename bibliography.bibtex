@misc{Jarvis2008,
abstract = {Jarvis A., H.I. Reuter, A. Nelson, E. Guevara, 2008, Hole-filled seamless SRTM data V4, International Centre for Tropical Agriculture (CIAT), available from http://srtm.csi.cgiar.org.},
author = {Jarvis, A and Reuter, H I and Nelson, A and Guevara, E},
booktitle = {International Centre for Tropical Agriculture (CIAT)},
pages = {available from http://srtm.csi.cgiar.org},
publisher = {nternational  Centre for Tropical  Agriculture (CIAT)},
title = {{Hole-filled seamless SRTM data V4}},
url = {http://srtm.csi.cgiar.org},
year = {2008}
}
@article{Vrac2007,
abstract = {Local-scale climate information is increasingly needed for the study of past, present and future climate changes. In this study we develop a non-linear statistical downscaling method to generate local temperatures and precipitation values from large-scale variables of a Earth System Model of Intermediate Complexity (here CLIMBER). Our statistical downscaling scheme is based on the concept of Generalized Additive Models (GAMs), capturing non-linearities via non-parametric techniques. Our GAMs are calibrated on the present Western Europe climate. For this region, annual GAMs (i.e. models based on 12 monthly values per location) are fitted by combining two types of large-scale explanatory variables: geographical (e.g. topographical information) and physical (i.e. entirely simulated by the CLIMBER model). To evaluate the adequacy of the non-linear transfer functions fitted on the present Western European climate, they are applied to different spatial and temporal large-scale conditions. Local projections for present North America and Northern Europe climates are obtained and compared to local observations. This partially addresses the issue of spatial robustness of our transfer functions by answering the question "does our statistical model remain valid when applied to large-scale climate conditions from a region different from the one used for calibration?". To asses their temporal performances, local projections for the Last Glacial Maximum period are derived and compared to local reconstructions and General Circulation Model outputs. Our downscaling methodology performs adequately for the Western Europe climate. Concerning the spatial and temporal evaluations, it does not behave as well for Northern America and Northern Europe climates because the calibration domain may be too different from the targeted regions. The physical explanatory variables alone are not capable of downscaling realistic values. However, the inclusion of geographical-type variables - such as altitude, advective continentality and moutains effect on wind (W-slope) - as GAM explanatory variables clearly improves our local projections.},
annote = {From Duplicate 1 (Non-linear statistical downscaling of present and LGM precipitation and temperatures over Europe - Vrac, M.; Paillard, D.; Naveau, P.)

From Duplicate 2 (Non-linear statistical downscaling of present and LGM precipitation and temperatures over Europe - Vrac, M.; Paillard, D.; Naveau, P.)

From Duplicate 4 (Non-linear statistical downscaling of present and LGM precipitation and temperatures over Europe - Vrac, M.; Paillard, D.; Naveau, P.)

Extracted Annotations (12/4/2014, 12:44:24 PM)
"The so-called downscaling methods aims at answering how to “zoom in” the information provided by large-scale systems (e.g. GCMs) into the required local-scale." (p. 670)
"Regional Climate Models (RCMs) provide a physically based approach (e.g. Liang et al., 2006). But, they are generally considered as equally (if not more) computationally intensive than GCMs." (p. 670)
"The class of Statistical Downscaling Methods (SDMs) represents a well-developped alternative that can bypass this computational obstacle. They are faster because they rely on empirical relationships between local-scale data (e.g. observations, reconstructions) and large-scale upper-air atmospheric variables (e.g. reanalysis data, GCM outputs). Another advantage resides in their probability foundation that allows to associate uncertainties more easily than with RCMs (e.g. Katz, 2002). The SDM proposed in this study belongs to the “transfer functions” SDM family (e.g. Zorita and von Storch, 1998; Snell et al., 2000; Huth, 2002). They directly link large–scale information to local–scale variable" (p. 670)
"One constraining assumption of any SDM resides in the hypothesis that the fitted transfer functions are stationary in time. While this assumption may hold reasonably well at a decadal or even centennial scale, its validity can be strongly challenged over thousands of year. This could lead to unrealistic statistical relationships and then to unrealistic projections of local variables." (p. 670)
"incorporating geographical variables that are, by nature, much less sensitive to changing large-scale conditions than physical ones can bring some stability and confidence to paleoclimatic downscaling projections." (p. 670)
"One of our goals in this paper is to present statistical models that have the flexibility of depicting any transfer function (linear and non-linear)." (p. 670)
"They will be non parametric, i.e. the relationship type would not be imposed a priori, but instead, driven by the data themselves." (p. 670)
"The local-scale temperature and precipitation data used for calibration and validation of our statistical model come from the “Climate Research Unit” database (CRU, New et al., 2000). The spatial resolution of these regularly gridded data is high (10', i.e. 1/6 degree)" (p. 671)
"Our goal is here to present a statistical model capable of regressing the values of local variables Y (temperatures or precipitation), called explained variables taken from the CRU database. The explanatory variables Xj – i.e. the variables used to explain Y – correspond to some of the large-scale values derived from CLIMBER outputs (see next Section). To go beyond the classical linear model, we opt to work with Generalized Additive Models (GAM). This allows us to infer non-linear relationships between the explained variable and the chosen explanatory variables." (p. 671)
"The following nine explanatory variables are considered as “physical” variables: specific (Q) and relative humidity (RH), sea level pressure (SLP), temperature (T), wind intensity in u (Wu) and v (Wv) directions, dew point temperature (Td), dew point temperature depression DT d=T d{\#}T and vertically integrated specific humidity (QI)" (p. 672)
"In addition to the “physical” variables, the following four “geographical” ones have been identified: elevation (elv), advective (Aco) and diffusive (Dco) continentality (see definition below), and W-slope (Wsl)" (p. 672)
"The diffusive continentality index Dco (between 0 and 1) corresponds to the shortest distance to the ocean." (p. 672)
"which are in opposition with the mean wind, i.e. penalizing an air-mass traveling against the wind (this would be inconsistent with our above assumptions for the continentality change over a given path). A simple way to perform this is to use the scalar product of the mean windU and the path direction unit vectorˆp (integrated over each path) Concerning the seasonality, shifts in seasonality are likely to occur over long time periods and applying monthly GAMs (i.e. one GAM per month) would potentially lead to unrealistic downscaled time series. This comes back to the issue of stationary raised in Sect. 1. To overcome this problem, we restrict our attention to annual GAMs, i.e. one single model calibrated over the entire twelve months (instead of a GAM per month), meaning that the same set of spline functions is now used for all months." (p. 673)
"Here, the Wslope corresponds to the mean zonal wind multiplied by the mean east-west slope over approximately 100 km" (p. 673)
"Remark that Aco and Wsl are not “purely geographical”, in the sense that they are not completely static. The classification into “physical” or “geographical” predictors is (for a part) arbitrary, and reflects more “classical” predictors versus our proposed original ones, respectively." (p. 673)
"It is common to disregard large-scale precipitation information (e.g. reanalysis data, GCM or EMIC outputs) when downscaling precipitation. Local precipitation usually have such a high spatial variability that large-scale precipitation data can be sometimes misleading (e.g., Vrac et al., 2007a)." (p. 673)
"the CLIMBER outputs are bi-linearly interpolated (e.g. Accadia et al., 2003) to the CRU resolution. Hence, the interpolated CLIMBER outputs can be considered as explanatory variables (i.e., predictor values) of the CRU precipitation and temperatures in our GAM." (p. 673)
"Overall, the final physical explanatory variables retained for our GAMs for explaining precipitation are Q, RH, T, Wu, Wv, Td, DTd, and QI" (p. 674)
"For explaining temperatures, the retained variables are slightly different: Q, RH, SLP, T, Wv, Td, DTd, and QI." (p. 674)
"Concerning our “geographical” variables, the following three has been conserved to explain both temperatures and precip- 19 the altitude (elv), the advective continentality (Aco) and the W-slope (Wsl)." (p. 674)
"Although with a relatively low percentage of variance explained (about 17.5{\%}), the log-precipitation residuals maps obtained from the geographical variables are paradoxically small (i.e. good) and reasonably centred on zero. This low percentage of variance comes from some relatively small errors distributed over the year and by stronger residuals (meaning stronger errors) in Southern Europe in July and August. Inferred precipitation" (p. 674)
"For this LGM downscaling, the retained explanatory variables are the geographical variables (elv, Aco, and Wsl), where one physical predictor is added, chosen based on the BIC results and relatively subjective choices. For precipitation, this variable is the CLIMBER sea level pressure (SLP), while for temperatures it is the CLIMBER temperature (T )." (p. 677)
"Although the goal of this paper is not to assess the CLIMBER simulations, the most surprising result may be the good agreement between CLIMBER and the local reconstructions. For both temperature and precipitation, the GAM downscaled values are realistic and generally brought some useful additional information. Indeed, even when CLIMBER is far away from the values to be retrieved/approximated, the downscaling process is sometimes capable of moving away from CLIMBER and getting closer to the reconstructions." (p. 678)
"Bardossy, A., Muster, H., Duckstein, L., and Bogardi, I.: Automatic classification of circulation patterns for stochastic precipitation modelling. Stochastic and Statistical Methods in Hydrology and Environmental Engineering, 1. Extreme Values: Floods and Droughts, 1994." (p. 681)},
author = {Vrac, M. and Marbaix, P. and Paillard, D. and Naveau, P.},
doi = {10.5194/cpd-3-899-2007},
file = {:home/nick/gdrive/Mendeley Desktop/Vrac et al. - 2007 - Non-linear statistical downscaling of present and LGM precipitation and temperatures over Europe.pdf:pdf;:home/nick/gdrive/Mendeley Desktop/Vrac et al. - 2007 - Non-linear statistical downscaling of present and LGM precipitation and temperatures over Europe(2).pdf:pdf;:home/nick/gdrive/Mendeley Desktop/Vrac et al. - 2007 - Non-linear statistical downscaling of present and LGM precipitation and temperatures over Europe(3).pdf:pdf;:home/nick/gdrive/Mendeley Desktop/Vrac et al. - 2007 - Non-linear statistical downscaling of present and LGM precipitation and temperatures over Europe(4).pdf:pdf;:home/nick/gdrive/Mendeley Desktop/Vrac et al. - 2007 - Non-linear statistical downscaling of present and LGM precipitation and temperatures over Europe(5).pdf:pdf},
isbn = {1814-9324},
journal = {Climate of the Past},
language = {English},
month = {dec},
number = {4},
pages = {669--682},
publisher = {Copernicus GmbH},
title = {{Non-linear statistical downscaling of present and LGM precipitation and temperatures over Europe}},
url = {http://www.clim-past.net/3/669/2007/},
volume = {3},
year = {2007}
}
@article{Wood2015,
abstract = {This paper discusses a general framework for smoothing parameter estimation for models with regular likelihoods constructed in terms of unknown smooth functions of covariates. Gaussian random effects and parametric terms may also be present. By construction the method is numerically stable and convergent, and enables smoothing parameter uncertainty to be quantified. The latter enables us to fix a well known problem with AIC for such models. The smooth functions are represented by reduced rank spline like smoothers, with associated quadratic penalties measuring function smoothness. Model estimation is by penalized like-lihood maximization, where the smoothing parameters controlling the extent of penalization are estimated by Laplace approximate marginal likelihood. The methods cover, for example, generalized additive models for non-exponential family responses (for example beta, ordered categorical, scaled t distribution, negative binomial and Tweedie distributions), generalized additive models for location scale and shape (for exam-ple two stage zero inflation models, and Gaussian location-scale models), Cox proportional hazards models and multivariate additive models. The framework reduces the implementation of new model classes to the coding of some standard derivatives of the log likelihood.},
archivePrefix = {arXiv},
arxivId = {arXiv:1511.03864v1},
author = {Wood, Simon N and Pya, Natalya and Benjamin, S},
doi = {10.1080/01621459.2016.1180986},
eprint = {arXiv:1511.03864v1},
file = {:home/nick/gdrive/Mendeley Desktop/Wood, Pya, Benjamin - 2015 - Smoothing parameter and model selection for general smooth models arXiv 1511 . 03864v1 stat . ME 12 Nov.pdf:pdf},
keywords = {()},
title = {{Smoothing parameter and model selection for general smooth models arXiv : 1511 . 03864v1 [ stat . ME ] 12 Nov 2015}},
year = {2015}
}
@article{Dee2011,
abstract = {ERA-Interim is the latest global atmospheric reanalysis produced by the European Centre for Medium-Range Weather Forecasts (ECMWF). The ERA-Interim project was conducted in part to prepare for a new atmospheric reanalysis to replace ERA-40, which will extend back to the early part of the twentieth century. This article describes the forecast model, data assimilation method, and input datasets used to produce ERA-Interim, and discusses the performance of the system. Special emphasis is placed on various difficulties encountered in the production of ERA-40, including the representation of the hydrological cycle, the quality of the stratospheric circulation, and the consistency in time of the reanalysed fields. We provide evidence for substantial improvements in each of these aspects. We also identify areas where further work is needed and describe opportunities and objectives for future reanalysis projects at ECMWF},
author = {Dee, D. P. and Uppala, S. M. and Simmons, A. J. and Berrisford, P. and Poli, P. and Kobayashi, S. and Andrae, U. and Balmaseda, M. A. and Balsamo, G. and Bauer, P. and Bechtold, P. and Beljaars, A. C. M. and van de Berg, L. and Bidlot, J. and Bormann, N. and Delsol, C. and Dragani, R. and Fuentes, M. and Geer, A. J. and Haimberger, L. and Healy, S. B. and Hersbach, H. and H{\'{o}}lm, E. V. and Isaksen, L. and K{\aa}llberg, P. and K{\"{o}}hler, M. and Matricardi, M. and Mcnally, A. P. and Monge-Sanz, B. M. and Morcrette, J.-J. J. and Park, B.-K. K. and Peubey, C. and de Rosnay, P. and Tavolato, C. and Th{\'{e}}paut, J.-N. and Vitart, F. and H??lm, E. V. and Isaksen, L. and K??llberg, P. and K??hler, M. and Matricardi, M. and Mcnally, A. P. and Monge-Sanz, B. M. and Morcrette, J.-J. J. and Park, B.-K. K. and Peubey, C. and de Rosnay, P. and Tavolato, C. and Th??paut, J. N. and Vitart, F.},
doi = {10.1002/qj.828},
file = {:home/nick/gdrive/Mendeley Desktop/Dee et al. - 2011 - The ERA-Interim reanalysis Configuration and performance of the data assimilation system.pdf:pdf},
isbn = {1477-870X},
issn = {00359009},
journal = {Quarterly Journal of the Royal Meteorological Society},
keywords = {4D-Var,4D‐Var,ERA-40,ERA‐40,Forecast model,Hydrological cycle,Observations,Stratospheric circulation,forecast model,hydrological cycle,observations,stratospheric circulation},
month = {apr},
number = {656},
pages = {553--597},
publisher = {John Wiley {\&} Sons, Ltd.},
title = {{The ERA-Interim reanalysis: Configuration and performance of the data assimilation system}},
url = {http://doi.wiley.com/10.1002/qj.828},
volume = {137},
year = {2011}
}
@article{Deblauwe2016,
author = {Deblauwe, V. and Droissart, V. and Bose, R. and Sonk{\'{e}}, B. and Blach-Overgaard, A. and Svenning, J.-C. and Wieringa, J. J. and Ramesh, B. R. and St{\'{e}}vart, T. and Couvreur, T. L. P.},
doi = {10.1111/geb.12426},
file = {:home/nick/gdrive/Mendeley Desktop/Deblauwe et al. - 2016 - Remotely sensed temperature and precipitation data improve species distribution modelling in the tropics.pdf:pdf},
issn = {1466822X},
journal = {Global Ecology and Biogeography},
keywords = {Association test,CHIRPS,GLM,MODIS,MaxEnt,TRMM,WorldClim,ecological niche model,habitat suitability,null model},
month = {apr},
number = {4},
pages = {443--454},
title = {{Remotely sensed temperature and precipitation data improve species distribution modelling in the tropics}},
url = {http://doi.wiley.com/10.1111/geb.12426},
volume = {25},
year = {2016}
}
@book{Wood2006a,
abstract = {Provides a complete resource for the penalized regression spline approach to GAMs, supported by the R package mgcv Covers linear, generalized linear, generalized additive, and corresponding mixed models within a single volume},
annote = {Extracted Annotations (2/8/2015, 2:31:15 PM)
"This flexibility and convenience comes at the cost of two new theoretical problems. It is necessary both to represent the smooth functions in some way and to choose how smooth they should be." (p. 132)

"A univariate function can be represented using a cubic spline. A cubic spline is a curve, made up of sections of cubic polynomial, joined together so that they are continuous in value as well as first and second derivatives (see figure 3.3). The points at which the sections join are known as the knots of the spline. For a conventional spline, the knots occur wherever there is a datum, but for the regression splines of interest here, the locations of the knots must be chosen. Typically the knots would either be evenly spaced through the range of observed x values, or placed at quantiles of the distribution of unique x values." (p. 135)

"An alternative to controlling smoothness by altering the basis dimension, is to keep the basis dimension fixed, at a size a little larger than it is believed could reasonably be necessary, but to control the model's smoothness by adding a "wiggliness" penalty to the least squares fitting objective." (p. 139)

"In practice, then, choice of basis dimension is something that probably has to remain a part of model specification. However, it is important to note that the exact size of basis dimension is really not that critical. The basis dimension is only setting an upper bound on the flexibility of a term: it is the smoothing parameter that controls the actual effective degrees of freedom" (p. 170)

"This isotropy is often considered to be desirable when modelling things as a smooth function of geographic co-ordinates‡, but it has some disadvantages. Chief among them is the difficulty of knowing how to scale predictors relative to one another, when both are arguments of the same smooth, but they are measured in fundamentally different units. For example, consider a smooth function of a single spatial co-ordinate and time: the implied relative importance of smoothness in time versus smoothness in space, is very different between a situation in which the units are metres and hours, compared to that in which the units are light-years and nanoseconds. One pragmatic approach is to scale all predictors into the unit square, as is often done in loess smoothing, but this is essentially arbitrary. A more satisfactory approach uses tensor product smooths." (p. 171)

"The p-values, calculated in this manner, behave correctly for un-penalized models, or models with known smoothing parameters, but when smoothing parameters have been estimated, the p-values are typically lower than they should be, meaning that the tests reject the null too readily. This is because smoothing parameter uncertainty has been neglected in the reference distributions used for testing. As a result these distributions are typically too narrow, so that they ascribe too low a probability to moderately high values of the test statistics. Limited simulation experience suggests that the p-values can be as little as half the correct value at around the 5{\%} level, when the null is true, although they may be more accurate than this in other circumstances. Note that this problem is in no way unique to GAMs. If you perform model selection on any model, and then hypothesis test using the selected model, the p-values associated with model terms will not be strictly correct, since they neglect model selection uncertainty. The advantage in performing model selection before hypothesis testing is that the elimination of unnecessary model degrees of freedom increases power." (p. 204)

"In practical terms, if these p-values suggest that a term is not needed in a model, then this is probably true, but if a term is deemed 'significant' it is important to be aware that this significance may be overstated. If hypothesis testing is a key aim of an analysis, then it may sometimes be preferable to base tests on overspecified unpenalized models, so that although the fits may be excessively wiggly, the p-values will be correct: the price to pay will be some loss of power." (p. 204)

"The coincidence of the confidence limits and the estimated straight line, at the point where the line passes through zero on the vertical axis, is a result of the identifiability constraints applied to the smooth terms∗" (p. 231)

"The points shown on the plots are partial residuals. These are simply the Pearson residuals added to the smooth terms evaluated at the appropriate covariate values" (p. 231)
"the number in each y-axis caption is the effective degrees of freedom of the term being plotted." (p. 231)

"The identifiability constraint is that the sum of the values of each curve, at the observed covariate values, must be zero: for a straight line, this condition exactly determines where the line must pass through zero, so there can be no uncertainty about this point." (p. 231)

"As you can see, the change in basis has made very little difference to the fit. Plots are in fact indistinguishable to those for ct1. This is re-assuring: it would be unfortunate if the model depended very strongly on details like the exact choice of basis." (p. 233)

"Another choice hidden, in the previous two model fits, is the choice of the dimension, k, of the basis used to represent smooth terms. In the previous two fits, the default, k = 10, was used. The choice of basis dimensions amounts to setting the maximum possible degrees of freedom allowed for each model term. The actual effective degrees of freedom, for each term, will usually be estimated from the data, by GCV or UBRE, but the upper limit on this estimate is k − 1: the basis dimension, less one degree of freedom due to the identifiability constraint on each smooth term." (p. 233)

"gam is not restricted to models containing only smooths of one predictor. In principle, smooths of any number of predictors are possible via two types of smooth. Within a model formula, s(), terms using the "tp" or "ts" bases, produce isotropic smooths of multiple predictors, while te() terms produce smooths of multiple predictors from tensor products of any bases available for use with s() (including mixtures of different bases). The tensor product smooths are invariant to linear rescaling of covariates, and can be quite computationally efficient." (p. 234)

"For data such as these, where the discretization (into voxels) is essentially arbitrary, there is clearly a case to be made for employing a model which includes local correlation in the 'error' terms. Chapter 6 covers methods for doing this," (p. 240)

"The following fits models based first on transforming the data, and then on use of the Gamma distribution." (p. 243)

"The major difference between m1 and m2 is in their biased-ness on different scales. The model of the transformed data is approximately unbiased on the 4th root of the response scale (approximately because the variance stabilization can only be approximate): this means that it is biased downwards on the response scale itself. The logGamma model is approximately unbiased on the response scale (only approximately because maximum penalized likelihood estimation is not generally unbiased, but is consistent). This can be seen if we look at the mean of the fitted values (response scale) for the two models, and compare this to the mean of the raw data: {\textgreater} mean(fitted(m1)ˆ4);mean(fitted(m2));mean(brain{\$}medFPQ) [1] 0.985554 {\#} m1 tends to under-estimate [1] 1.212545 {\#} m2 substantially better [1] 1.250302 Clearly, if the response scale is the scale of prime interest, then the Gamma model is to be preferred to the a model based on normality of the transformed data." (p. 243)

"). Following section 4.9.3, we could improve matters by approximating f( ˆ) by its bootstrap sampling distribution, and then using the fact that f(\beta) = f(\beta|ˆ\lambda)f(ˆ\lambda), to simulate from an approximate version of f(\beta). The following code does just that, and plots the results in the lower right panel of figure 5.20." (p. 270)

"As usual, the (deviance) residuals should be plotted against fitted values to check model assumptions, but conventional residual plots are unlikely to pick up one potential problem with spatial data: namely spatial auto-correlation in the residuals, and consequent violation of the independence assumption. To check this, it is useful to examine the variogram of the residuals, and the geoR package has convenient functions for doing this. library(geoR) coords{\textless}-matrix(0,1004,2);coords[,1]{\textless}-ba{\$}x;coords[,2]{\textless}-ba{\$}y gb{\textless}-list(data=residuals(m10,type="d"),coords=coords) plot(variog(gb,max.dist=1e5)) plot(fitted(m10),residuals(m10))" (p. 272)},
author = {Wood, Simon N},
doi = {10.1111/j.1541-0420.2006.00574.x},
file = {:home/nick/gdrive/Mendeley Desktop/Wood - 2006 - Generalized Additive Models An Introduction with R.pdf:pdf},
isbn = {978-1-58488-474-3},
pmid = {17156276},
publisher = {Chapman and Hall/CRC press},
shorttitle = {Generalized additive models},
title = {{Generalized Additive Models: An Introduction with R}},
url = {http://opus.bath.ac.uk/7011/},
year = {2006}
}
@article{Barton2017,
abstract = {{\textcopyright} 2017 Elsevier Ltd. The period spanning the Last Glacial Maximum through early Holocene encompasses dramatic and rapid environmental changes that offered both increased risk and new opportunities to human populations of the Mediterranean zone. The regional effects of global climate change varied spatially with latitude, topography, and distance from a shifting coastline; and human adaptations to these changes played out at these regional scales. To better understand the spatial and temporal dynamics of climate change and human social-ecological-technological systems (or SETS) during the transition from full glacial to interglacial, we carried out a meta-analysis of archaeological and paleoenvironmental datasets across the western Mediterranean region. We compiled information on prehistoric technology, land-use, and hunting strategies from 291 archaeological assemblages, recovered from 122 sites extending from southern Spain, through Mediterranean France, to northern and peninsular Italy, as well as 2,386 radiocarbon dates from across this region. We combine these data on human ecological dynamics with paleoenvironmental information derived from global climate models, proxy data, and estimates of coastlines modeled from sea level estimates and digital terrain. The LGM represents an ecologically predictable period for over much of the western Mediterranean, while the remainder of the Pleistocene was increasingly unpredictable, making it a period of increased ecological risk for hunter-gatherers. In response to increasing spatial and temporal uncertainty, hunter-gatherers reorganized different constituents of their SETS, allowing regional populations to adapt to these conditions up to a point. Beyond this threshold, rapid environmental change resulted in significant demographic change in Mediterranean hunter-gatherer populations.},
author = {Barton, C.M. Michael and {Aura Tortosa}, J.E. Emili and Garcia-Puchol, Oreto and Riel-Salvatore, J.G. Julien G. and Gauthier, Nicolas and {Vadillo Conesa}, Margarita and {Pothier Bouchard}, Genevi{\`{e}}ve},
doi = {10.1016/j.quascirev.2017.09.015},
file = {:home/nick/gdrive/Mendeley Desktop/Barton et al. - 2017 - Risk and resilience in the late glacial A case study from the western Mediterranean.pdf:pdf},
issn = {02773791},
journal = {Quaternary Science Reviews},
keywords = {Adaptation,Archaeology,Demography,Environmental uncertainty,Human ecology,Hunter-gatherers,Paleoclimate models,Paleolithic,Pleistocene-Holocene,Western Mediterranean,pleistocene-holocene},
pages = {1--17},
title = {{Risk and resilience in the late glacial: A case study from the western Mediterranean}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0277379117302615},
year = {2017}
}
@article{Braconnot2012,
abstract = {There is large uncertainty about the magnitude of warming and how rainfall patterns will change in response to any given scenario of future changes in atmospheric composition and land use. The models used for future climate projections were developed and calibrated using climate observations from the past 40 years. The geologic record of environmental responses to climate changes provides a unique opportunity to test model performance outside this limited climate range. Evaluation of model simulations against palaeodata shows that models reproduce the direction and large-scale patterns of past changes in climate, but tend to underestimate the magnitude of regional changes. As part of the effort to reduce model-related uncertainty and produce more reliable estimates of twenty-first century climate, the Palaeoclimate Modelling Intercomparison Project is systematically applying palaeoevaluation techniques to simulations of the past run with the models used to make future projections. This evaluation will provide assessments of model performance, including whether a model is sufficiently sensitive to changes in atmospheric composition, as well as providing estimates of the strength of biosphere and other feedbacks that could amplify the model response to these changes and modify the characteristics of climate variability.},
author = {Braconnot, Pascale and Harrison, Sandy P. and Kageyama, Masa and Bartlein, Patrick J. and Masson-Delmotte, Valerie and Abe-Ouchi, Ayako and Otto-Bliesner, Bette and Zhao, Yan},
doi = {10.1038/nclimate1456},
isbn = {1758-678X},
issn = {1758-678X},
journal = {Nature Climate Change},
keywords = {Atmospheric science,Earth Sciences,Modelling and statistics,palaeoclimate},
language = {en},
mendeley-tags = {Atmospheric science,Earth Sciences,Modelling and statistics,palaeoclimate},
month = {jun},
number = {6},
pages = {417--424},
title = {{Evaluation of climate models using palaeoclimatic data}},
url = {http://www.nature.com/nclimate/journal/v2/n6/full/nclimate1456.html},
volume = {2},
year = {2012}
}
